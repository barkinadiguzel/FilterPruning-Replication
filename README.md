# ðŸª“ FilterPruning-Replication â€“ Structured Filter Removal for Efficient ConvNets

This repository presents a **PyTorch-based replication** of  
**Pruning Filters for Efficient ConvNets** (ICLR 2017).
The goal is **not benchmarking accuracy**, but **faithfully translating the paperâ€™s theoretical pruning logic into a clean, inspectable implementation**, without relying on data or full training loops.

- Filter importance measured via **L1-norm of convolutional kernels** âŸ  
- **Structured filter pruning** instead of unstructured sparsity âŸ¡  
- Architecture-aware pruning strategies (independent & greedy) âŸ¢  

**Paper reference:**  [Pruning Filters for Efficient ConvNets â€“ Li et al., 2017](https://arxiv.org/abs/1608.08710) ðŸœ„

---

## ðŸŒ— Overview â€“ Filter Pruning Pipeline

![Filter Pruning Overview](images/figmix.jpg)

Core intuition:

> Filters with small weight magnitudes contribute weak feature maps and can be safely removed.

Condensed pipeline:

1. Start from a **predefined CNN architecture** (e.g. VGG-style or ResNet-style blocks).
2. For each convolutional layer, compute **filter importance scores** using L1-norm.
3. Rank filters and **prune the weakest ones** according to layer or stage sensitivity.
4. Physically remove:
   - the selected filters,
   - their output feature maps,
   - and the corresponding kernels in the next layer.
5. (Optional) Repeat pruning iteratively or retrain if desired.

The result is a **smaller, denser, computation-friendly network**, not a sparse one.

---

## ðŸ§® Filter Importance via L1-Norm

For a convolutional filter $F_j$ with kernel weights $w$, importance is defined as:

$$
\|F_j\|_1 = \sum_{l=1}^{n_i} \sum |w_l|
$$

Where:
- $n_i$ is the number of input channels,
- all filters within the same layer share the same $n_i$.

This score approximates the **expected activation strength** of the corresponding output feature map.

Key insight:
- Filters with small L1-norm tend to produce weak activations,
- Removing them minimally affects representational capacity.

This makes L1-norm a **data-free, architecture-agnostic pruning criterion**.

---

## ðŸ§  Layer Sensitivity & Pruning Strategy

Not all layers react equally to pruning.

- Layers with **steep L1 distributions** tolerate aggressive pruning.
- Layers with **flat distributions** are sensitive and require conservative pruning.

To reflect this:
- Pruning ratios are applied **stage-wise**, not per-layer.
- Layers operating on small spatial resolutions often allow higher pruning rates.

Two pruning strategies are implemented:

- **Independent pruning**  
  Each layer is pruned in isolation, ignoring previous pruning decisions.

- **Greedy pruning**  
  Filters removed in earlier layers are excluded from later importance calculations, producing a more holistic result when pruning heavily.

---

## ðŸ§± Structured vs Sparse Pruning

This method intentionally avoids sparse weight matrices.

Instead of zeroing individual weights:
- Entire filters are removed,
- Feature maps disappear,
- Kernel tensors shrink.

This leads to:
- Real FLOP reduction,
- Hardware-friendly execution,
- No dependency on sparse libraries.

In short: **filters vanish, not just their values**.

---

## ðŸ“¦ Repository Structure

```bash
FilterPruning-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py          # Conv2d wrapper (pruning-safe)
â”‚   â”‚   â”œâ”€â”€ activation.py          # Activation utilities
â”‚   â”‚   â””â”€â”€ normalization.py       # Optional BN compatibility
â”‚   â”‚
â”‚   â”œâ”€â”€ backbone/
â”‚   â”‚   â””â”€â”€ vgg_blocks.py          # VGG-style convolutional blocks
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ prunable_cnn.py        # CNN designed for filter removal
â”‚   â”‚
â”‚   â”œâ”€â”€ pruning/
â”‚   â”‚   â”œâ”€â”€ filter_score.py        # L1-norm computation (core math)
â”‚   â”‚   â”œâ”€â”€ prune_filters.py       # Physical filter & channel removal
â”‚   â”‚   â””â”€â”€ strategies.py          # Independent vs greedy pruning
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚   â””â”€â”€ classification_loss.py          
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â””â”€â”€ flop_counter.py        # FLOP comparison before/after pruning
â”‚   â”‚
â”‚   â””â”€â”€ config.py                  # Stage-wise pruning ratios
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg                 # Filter weight distribution & pruning flow
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ðŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
